# beamai_agent

English | [中文](README.md)

A simple ReAct Agent implementation with support for tool calling, Middleware, checkpoints, and multi-turn conversations.

## Features

- **ReAct Pattern**: Reasoning + Acting loop execution
- **Tool Calling**: Support for custom tools and built-in tools
- **Middleware System**: Extensible interceptor mechanism
- **Checkpoint Persistence**: Save and restore Agent state
- **Callback System**: Listen to execution events
- **Multi-turn Conversation**: Automatic conversation history management

## Module Overview

### Core Modules
- **beamai_agent** - Main module, stateless function implementation
- **beamai_agent_init** - Initialization logic
- **beamai_agent_runner** - Executor
- **beamai_agent_callbacks** - Callback handling
- **beamai_agent_checkpoint** - Checkpoint management

### Middleware Modules
- **beamai_middleware** - Middleware behavior definition
- **beamai_middleware_runner** - Middleware executor
- **beamai_middleware_presets** - Preset configurations
- **middleware_call_limit** - Call limiting
- **middleware_summarization** - Context summarization
- **middleware_human_approval** - Human approval
- **middleware_tool_retry** - Tool retry
- **middleware_model_retry** - Model retry
- **middleware_model_fallback** - Model fallback
- **middleware_pii_detection** - PII detection

### Coordinator Modules
- **beamai_coordinator** - Multi-Agent coordination
- **beamai_coordinator_common** - Coordinator common functions
- **beamai_nodes** - Node definitions

### beamai_coordinator API

The coordinator is used to manage multiple Agents working together, supporting two modes: Pipeline (sequential) and Orchestrator (orchestration).

```erlang
%% Start coordinator
beamai_coordinator:start_link(Id, Opts) -> {ok, Pid} | {error, Reason}.
beamai_coordinator:start_pipeline(Id, Opts) -> {ok, Pid} | {error, Reason}.
beamai_coordinator:start_orchestrator(Id, Opts) -> {ok, Pid} | {error, Reason}.

%% Stop coordinator (Note: parameter is Pid, not Id)
beamai_coordinator:stop(CoordinatorPid) -> ok | {error, Reason}.

%% Get Workers (parameter is Pid)
beamai_coordinator:get_workers(CoordinatorPid) -> {ok, #{Name => WorkerPid}} | {error, Reason}.
beamai_coordinator:get_worker(CoordinatorPid, WorkerName) -> {ok, WorkerPid} | {error, Reason}.

%% Delegate tasks (parameter is Pid)
beamai_coordinator:delegate(CoordinatorPid, WorkerName, Task) -> {ok, Result} | {error, Reason}.
beamai_coordinator:delegate_parallel(CoordinatorPid, [WorkerName], Task) -> {ok, #{Name => Result}}.
```

**Note:** Starting from v2.1, the `beamai_coordinator` API parameters changed from using `Id` to using `CoordinatorPid`. Coordinator metadata is stored in the Agent's `meta` field, rather than in a shared ETS table.

## API Documentation

### beamai_agent (Pure Function API)

```erlang
%% Core API
beamai_agent:new(Config) -> {ok, State} | {error, Reason}.
beamai_agent:run(State, Message) -> {ok, Result, NewState} | {error, Reason}.
beamai_agent:run(State, Message, Opts) -> {ok, Result, NewState} | {error, Reason}.
beamai_agent:restore_from_memory(Config, Memory) -> {ok, State} | {error, Reason}.

%% State Query
beamai_agent:get_messages(State) -> [Message].
beamai_agent:get_full_messages(State) -> [Message].
beamai_agent:get_scratchpad(State) -> [Step].
beamai_agent:get_context(State) -> Context.
beamai_agent:get_context(State, Key) -> Value | undefined.
beamai_agent:get_context(State, Key, Default) -> Value.

%% State Modification
beamai_agent:set_context(State, Context) -> NewState.
beamai_agent:update_context(State, Updates) -> NewState.
beamai_agent:put_context(State, Key, Value) -> NewState.
beamai_agent:clear_messages(State) -> NewState.
beamai_agent:clear_scratchpad(State) -> NewState.
```

### Configuration Structure

```erlang
%% First create LLM configuration (must use llm_client:create/2)
LLM = llm_client:create(openai, #{
    model => <<"gpt-4">>,
    api_key => list_to_binary(os:getenv("OPENAI_API_KEY"))
}),

%% Agent configuration
Config = #{
    %% Required
    system_prompt => <<"You are a helpful assistant.">>,

    %% LLM configuration (must be created using llm_client:create/2)
    llm => LLM,

    %% Tool list (optional, recommended to use beamai_tool_registry to build)
    tools => beamai_tool_registry:from_config(#{
        tools => [MyCustomTool],
        providers => [beamai_tool_provider_builtin]
    }),

    %% Checkpoint storage (optional)
    storage => Memory,  %% beamai_memory instance

    %% Auto checkpoint (optional)
    auto_checkpoint => true,

    %% Maximum iterations (optional)
    max_iterations => 10,

    %% Middleware configuration (optional)
    middlewares => [
        {middleware_call_limit, #{max_model_calls => 20}},
        {middleware_summarization, #{window_size => 20}}
    ],

    %% Callback functions (optional)
    callbacks => #{
        on_llm_start => fun(Prompts, Meta) -> ok end,
        on_llm_end => fun(Response, Meta) -> ok end,
        on_tool_use => fun(ToolName, Args, Meta) -> ok end
    },

    %% User context initial values (optional)
    context => #{
        counter => 0,
        items => []
    },

    %% User context field reducer configuration (optional)
    context_reducers => #{
        %% Normal reducer: same-key merge
        <<"items">> => fun graph_state_reducer:append_reducer/2,
        %% Transform reducer: accumulate counter_incr to counter
        <<"counter_incr">> => {transform, <<"counter">>, fun graph_state_reducer:increment_reducer/2}
    }
}.
```

## Context Reducers

Context Reducers allow configuring custom merge strategies for user context fields, particularly useful during concurrent updates.

### Reducer Types

Two reducer formats are supported:

1. **Normal Reducer** - Same-key merge
```erlang
%% Configuration
context_reducers => #{
    <<"items">> => fun graph_state_reducer:append_reducer/2
}

%% Node update
graph_state:update_context(State, #{items => [new_item]})
%% Result: new_item is appended to items list
```

2. **Transform Reducer** - Read delta from source key, apply to target key, source key not retained
```erlang
%% Configuration
context_reducers => #{
    <<"counter_incr">> => {transform, <<"counter">>, fun graph_state_reducer:increment_reducer/2}
}

%% Node update
graph_state:update_context(State, #{counter_incr => 5})
%% Result: counter += 5, counter_incr does not appear in final state
```

### Built-in Reducers

| Reducer | Behavior | Use Case |
|---------|----------|----------|
| `append_reducer` | List append | messages, items |
| `merge_reducer` | Deep map merge | Nested objects |
| `increment_reducer` | Numeric accumulation | Counters |
| `last_write_win_reducer` | New value overwrites old | Default strategy |

### Usage Example

```erlang
Config = #{
    system_prompt => <<"You are a counter assistant.">>,
    llm => LLM,
    context => #{
        counter => 0,
        history => []
    },
    context_reducers => #{
        %% Counter increment: counter_incr value accumulates to counter
        <<"counter_incr">> => {transform, <<"counter">>, fun graph_state_reducer:increment_reducer/2},
        %% History: append mode
        <<"history">> => fun graph_state_reducer:append_reducer/2
    }
},

{ok, State} = beamai_agent:new(Config).
```

## Middleware System

Middleware are interceptors during Agent execution that can intervene at various stages.

### Lifecycle Hooks

```
before_agent -> [before_model -> LLM -> after_model -> before_tools -> Tools -> after_tools]* -> after_agent
```

| Hook | Trigger Timing | Typical Use |
|------|----------------|-------------|
| `before_agent` | Before Agent starts | Initialization |
| `before_model` | Before LLM call | Check limits, modify messages |
| `after_model` | After LLM returns | Process response |
| `before_tools` | Before tool execution | Human approval |
| `after_tools` | After tool execution | Result validation |
| `after_agent` | After Agent ends | Cleanup, logging |

### Using Preset Configurations

```erlang
%% Default configuration
Config = #{
    middlewares => beamai_middleware_presets:default()
}.

%% Production environment configuration
Config = #{
    middlewares => beamai_middleware_presets:production()
}.

%% Human approval configuration
Config = #{
    middlewares => beamai_middleware_presets:human_in_loop()
}.
```

### Built-in Middleware

| Middleware | Description |
|------------|-------------|
| `middleware_call_limit` | Limit model/tool call count |
| `middleware_summarization` | Automatically compress long conversations |
| `middleware_human_approval` | Human confirmation before tool execution |
| `middleware_tool_retry` | Automatic retry on tool failure |
| `middleware_model_retry` | Automatic retry on LLM failure |
| `middleware_model_fallback` | Switch to backup model on primary failure |
| `middleware_pii_detection` | Detect sensitive information |

### Custom Middleware

```erlang
-module(my_middleware).
-behaviour(beamai_middleware).

-export([init/1, before_model/2]).

%% Initialization
init(Opts) ->
    #{max_calls => maps:get(max_calls, Opts, 10)}.

%% Check before LLM call
before_model(State, #{max_calls := Max} = _MwState) ->
    Count = graph_state:get(State, call_count, 0),
    case Count >= Max of
        true -> {halt, call_limit_exceeded};
        false -> {update, #{call_count => Count + 1}}
    end.
```

### Middleware Return Values

```erlang
ok                              %% Continue execution
{update, #{key => value}}       %% Update state
{goto, model | tools | '__end__'}  %% Jump to
{halt, Reason}                  %% Abort
{interrupt, Action}             %% Interrupt and wait for confirmation
```

Detailed documentation: [Middleware System](../../doc/MIDDLEWARE.md)

## Usage Examples

### Basic Usage

```erlang
%% Create LLM configuration
LLM = llm_client:create(openai, #{
    model => <<"gpt-4">>,
    api_key => list_to_binary(os:getenv("OPENAI_API_KEY"))
}),

%% Configuration
Config = #{
    system_prompt => <<"You are a helpful assistant.">>,
    llm => LLM
},

%% Create Agent state
{ok, State0} = beamai_agent:new(Config),

%% Multi-turn conversation
{ok, _Result1, State1} = beamai_agent:run(State0, <<"Hello!">>),
{ok, _Result2, _State2} = beamai_agent:run(State1, <<"What can you do?">>).
```

### Using Tools

```erlang
%% Create LLM configuration
LLM = llm_client:create(openai, #{
    model => <<"gpt-4">>,
    api_key => list_to_binary(os:getenv("OPENAI_API_KEY"))
}),

%% Define calculator tool
CalculatorTool = #{
    name => <<"calculator">>,
    description => <<"Perform mathematical calculations">>,
    parameters => #{
        type => object,
        properties => #{
            <<"expression">> => #{type => string}
        },
        required => [<<"expression">>]
    },
    handler => fun(#{<<"expression">> := Expr}) ->
        %% Simple evaluation (needs safe handling in real applications)
        {ok, Tokens, _} = erl_scan:string(binary_to_list(Expr)),
        {ok, Parsed} = erl_parse:parse_exprs(Tokens),
        {value, Result, _} = erl_eval:exprs(Parsed, []),
        {ok, #{result => Result}}
    end
},

%% Use Registry to build tool list
Tools = beamai_tool_registry:from_config(#{
    tools => [CalculatorTool]
}),

Config = #{
    system_prompt => <<"You are an assistant that can do math.">>,
    llm => LLM,
    tools => Tools
},

{ok, State} = beamai_agent:new(Config),
{ok, Result, _NewState} = beamai_agent:run(State, <<"What is 123 * 456?">>).
```

### Using Memory Persistence

```erlang
%% Create LLM configuration
LLM = llm_client:create(openai, #{
    model => <<"gpt-4">>,
    api_key => list_to_binary(os:getenv("OPENAI_API_KEY"))
}),

%% Create storage
{ok, _} = beamai_store_ets:start_link(my_store, #{}),
{ok, Memory} = beamai_memory:new(#{context_store => {beamai_store_ets, my_store}}),

%% Create Agent with storage
Config = #{
    system_prompt => <<"You are a helpful assistant.">>,
    llm => LLM,
    storage => Memory
},

{ok, State0} = beamai_agent:new(Config),

%% Conversation (checkpoint auto-saved)
{ok, _, State1} = beamai_agent:run(State0, <<"Remember: my name is Alice.">>),
{ok, _, _State2} = beamai_agent:run(State1, <<"What's the weather?">>),

%% Restore session from Memory
{ok, RestoredState} = beamai_agent:restore_from_memory(#{llm => LLM}, Memory),
Messages = beamai_agent:get_messages(RestoredState).
```

### Using Alibaba Cloud Bailian

```erlang
%% Create Bailian configuration (Tongyi Qianwen)
LLM = llm_client:create(bailian, #{
    model => <<"qwen3-max">>,
    api_key => list_to_binary(os:getenv("BAILIAN_API_KEY"))
}),

Config = #{
    system_prompt => <<"You are a helpful AI assistant.">>,
    llm => LLM
},

{ok, State} = beamai_agent:new(Config),
{ok, Result, _} = beamai_agent:run(State, <<"Hello! Please introduce yourself.">>).
```

### Using Zhipu AI

```erlang
%% Create Zhipu AI configuration (using Anthropic-compatible interface)
LLM = llm_client:create(anthropic, #{
    model => <<"glm-4.7">>,
    api_key => list_to_binary(os:getenv("ZHIPU_API_KEY")),
    base_url => <<"https://open.bigmodel.cn/api/anthropic">>
}),

Config = #{
    system_prompt => <<"You are a helpful AI assistant.">>,
    llm => LLM
},

{ok, State} = beamai_agent:new(Config),
{ok, Result, _} = beamai_agent:run(State, <<"Hello! Please introduce yourself.">>).
```

### Using Coordinator (Pipeline Mode)

```erlang
%% Create LLM configuration
LLM = llm_client:create(bailian, #{
    model => <<"qwen-plus">>,
    api_key => list_to_binary(os:getenv("BAILIAN_API_KEY"))
}),

%% Define Agents in the Pipeline (translation pipeline)
Agents = [
    #{
        name => <<"translator">>,
        system_prompt => <<"You are a translation expert, translating Chinese to English.">>
    },
    #{
        name => <<"polisher">>,
        system_prompt => <<"You are an English polishing expert, optimizing English expressions.">>
    }
],

%% Start Pipeline coordinator
{ok, Coordinator} = beamai_coordinator:start_pipeline(<<"translation_pipeline">>, #{
    agents => Agents,
    llm => LLM
}),

%% Directly delegate to specified worker
{ok, Result} = beamai_coordinator:delegate(Coordinator, <<"translator">>, <<"Hello World">>),

%% Get all workers
{ok, Workers} = beamai_coordinator:get_workers(Coordinator),

%% Stop coordinator
beamai_coordinator:stop(Coordinator).
```

### Using Coordinator (Orchestrator Mode)

```erlang
%% Define multi-expert Agents
Agents = [
    #{name => <<"tech_expert">>, system_prompt => <<"You are a technology expert.">>},
    #{name => <<"business_expert">>, system_prompt => <<"You are a business expert.">>}
],

%% Start Orchestrator coordinator
{ok, Coordinator} = beamai_coordinator:start_orchestrator(<<"expert_panel">>, #{
    agents => Agents,
    llm => LLM
}),

%% Delegate to multiple workers in parallel
{ok, Results} = beamai_coordinator:delegate_parallel(
    Coordinator,
    [<<"tech_expert">>, <<"business_expert">>],
    <<"Analyze the impact of AI on the industry">>
),

%% Results = #{<<"tech_expert">> => {ok, "..."}, <<"business_expert">> => {ok, "..."}}

beamai_coordinator:stop(Coordinator).
```

## Dependencies

- beamai_core
- beamai_llm
- beamai_memory

## License

Apache-2.0
