%%%-------------------------------------------------------------------
%%% @doc Process-native Agent LLM Step
%%%
%%% 实现 beamai_step_behaviour，作为 Process-native Agent 的 LLM 调用环节。
%%% 与 beamai_agent 的内部 tool loop 不同，本模块将每次 LLM 调用
%%% 作为独立的 step 激活，tool 调用通过 Process 事件路由到
%%% beamai_process_agent_tool_step 执行。
%%%
%%% == 工作流程 ==
%%%   1. 接收 user_message 或 tool_results 输入
%%%   2. 构建消息列表，调用 LLM（通过 Context 中的 Kernel）
%%%   3. 若 LLM 返回 tool_calls: 发射 tool_request 事件
%%%   4. 若 LLM 返回文本: 发射 agent_done 事件
%%%
%%% == 配置 ==
%%% ```
%%% #{
%%%     system_prompt => binary(),          %% 系统提示词
%%%     max_tool_iterations => pos_integer(),%% 最大 tool 迭代次数（默认 10）
%%%     output_event => atom()              %% 完成事件名（默认 agent_done）
%%% }
%%% ```
%%%
%%% == 输入 ==
%%%   - user_message: binary() — 用户消息
%%%   - tool_results: [map()] — tool 执行结果（来自 tool_step）
%%%
%%% == 输出事件 ==
%%%   - tool_request: #{tool_calls => [...], assistant_msg => map()}
%%%   - agent_done: #{response => binary(), tool_calls_made => [...], turn_count => integer()}
%%%
%%% @end
%%%-------------------------------------------------------------------
-module(beamai_process_agent_llm_step).

-behaviour(beamai_step_behaviour).

-export([init/1, can_activate/2, on_activate/3]).

%%====================================================================
%% beamai_step_behaviour 回调
%%====================================================================

init(Config) ->
    SystemPrompt = maps:get(system_prompt, Config, undefined),
    MaxIter = maps:get(max_tool_iterations, Config, 10),
    OutputEvent = maps:get(output_event, Config, agent_done),
    {ok, #{
        system_prompt => SystemPrompt,
        messages => [],
        iteration => 0,
        max_iterations => MaxIter,
        output_event => OutputEvent,
        tool_calls_made => [],
        all_tool_calls => [],
        turn_count => 0,
        last_response => <<>>
    }}.

can_activate(Inputs, _State) ->
    maps:is_key(user_message, Inputs) orelse maps:is_key(tool_results, Inputs).

on_activate(Inputs, State, Context) ->
    #{system_prompt := SysPrompt, messages := History,
      iteration := Iter, max_iterations := MaxIter,
      output_event := OutputEvent, tool_calls_made := PrevToolCalls,
      all_tool_calls := AllPrevToolCalls,
      turn_count := TurnCount} = State,

    %% 检查迭代次数
    case Iter >= MaxIter of
        true ->
            {error, {max_tool_iterations, PrevToolCalls}};
        false ->
            %% 获取 Kernel
            case beamai_context:get_kernel(Context) of
                undefined ->
                    {error, no_kernel_in_context};
                Kernel ->
                    %% 根据输入类型构建消息
                    {NewMessages, IsNewTurn} = build_messages(Inputs, History, SysPrompt),

                    %% 构建 chat 选项
                    ChatOpts = beamai_agent_utils:build_chat_opts(Kernel, #{}),

                    %% 调用 LLM
                    case beamai_kernel:invoke_chat(Kernel, NewMessages, ChatOpts) of
                        {ok, #{tool_calls := TCs} = _Response, _Ctx}
                          when is_list(TCs), TCs =/= [] ->
                            %% LLM 请求 tool 调用
                            AssistantMsg = #{role => assistant, content => null, tool_calls => TCs},
                            UpdatedMessages = NewMessages ++ [AssistantMsg],

                            %% 记录 tool calls
                            NewToolCalls = lists:map(fun(TC) ->
                                {_Id, Name, Args} = beamai_function:parse_tool_call(TC),
                                #{name => Name, args => Args}
                            end, TCs),

                            NewState = State#{
                                messages => UpdatedMessages,
                                iteration => Iter + 1,
                                tool_calls_made => PrevToolCalls ++ NewToolCalls,
                                all_tool_calls => AllPrevToolCalls ++ NewToolCalls
                            },

                            EventData = #{
                                tool_calls => TCs,
                                assistant_msg => AssistantMsg
                            },
                            Event = beamai_process_event:new(tool_request, EventData),
                            {ok, #{events => [Event], state => NewState}};

                        {ok, Response, _Ctx} ->
                            %% LLM 返回文本响应
                            Content = beamai_agent_utils:extract_content(Response),
                            AssistantMsg = #{role => assistant, content => Content},
                            UserMsg = extract_user_msg(Inputs),
                            FinalMessages = case IsNewTurn of
                                true ->
                                    %% 保留 user + assistant 到历史
                                    History ++ [UserMsg, AssistantMsg];
                                false ->
                                    %% tool loop 结束，只加 assistant
                                    NewMessages ++ [AssistantMsg]
                            end,

                            NewTurnCount = TurnCount + 1,

                            NewState = State#{
                                messages => FinalMessages,
                                iteration => 0,
                                tool_calls_made => [],
                                turn_count => NewTurnCount,
                                last_response => Content
                            },

                            EventData = #{
                                response => Content,
                                tool_calls_made => PrevToolCalls,
                                turn_count => NewTurnCount,
                                finish_reason => maps:get(finish_reason, Response, <<>>)
                            },
                            Event = beamai_process_event:new(OutputEvent, EventData),
                            {ok, #{events => [Event], state => NewState}};

                        {error, Reason} ->
                            {error, {llm_call_failed, Reason}}
                    end
            end
    end.

%%====================================================================
%% 内部函数
%%====================================================================

%% @private 根据输入类型构建消息列表
build_messages(#{user_message := UserMsg}, History, SysPrompt) ->
    Sys = sys_messages(SysPrompt),
    UserMessage = #{role => user, content => UserMsg},
    {Sys ++ History ++ [UserMessage], true};
build_messages(#{tool_results := ToolResults}, History, SysPrompt) ->
    %% tool_results 来自 tool_step，追加到当前消息（History 已含 assistant_msg）
    Sys = sys_messages(SysPrompt),
    ToolMsgs = format_tool_results(ToolResults),
    {Sys ++ History ++ ToolMsgs, false};
build_messages(Inputs, History, SysPrompt) ->
    %% 尝试从任意 key 提取用户消息
    case find_user_message(Inputs) of
        {ok, Msg} ->
            Sys = sys_messages(SysPrompt),
            UserMessage = #{role => user, content => Msg},
            {Sys ++ History ++ [UserMessage], true};
        error ->
            Sys = sys_messages(SysPrompt),
            {Sys ++ History, false}
    end.

sys_messages(undefined) -> [];
sys_messages(<<>>) -> [];
sys_messages(Prompt) -> [#{role => system, content => Prompt}].

format_tool_results(Results) when is_list(Results) ->
    lists:map(fun(#{tool_call_id := Id, result := R}) ->
        #{role => tool, tool_call_id => Id, content => ensure_binary(R)};
    (#{tool_call_id := Id, content := C}) ->
        #{role => tool, tool_call_id => Id, content => ensure_binary(C)};
    (Other) ->
        Other
    end, Results).

extract_user_msg(#{user_message := Msg}) ->
    #{role => user, content => Msg};
extract_user_msg(_) ->
    #{role => user, content => <<>>}.

find_user_message(Inputs) ->
    Values = maps:values(Inputs),
    find_binary(Values).

find_binary([]) -> error;
find_binary([V | _]) when is_binary(V) -> {ok, V};
find_binary([#{user_message := Msg} | _]) -> {ok, Msg};
find_binary([_ | Rest]) -> find_binary(Rest).

ensure_binary(V) when is_binary(V) -> V;
ensure_binary(V) -> beamai_function:encode_result(V).
