# Pregel 架构选择：Worker 管理多顶点 vs 每顶点一进程

## 概述

本文档分析为什么 Pregel 选择让一个 Worker 进程管理多个顶点，而不是每个顶点一个 Erlang 进程。

## 两种方案对比

### 方案 A：每个 Vertex 一个进程

```
┌─────────────────────────────────────────────────────────────────┐
│                      Master                                     │
├─────────────────────────────────────────────────────────────────┤
│                          │                                      │
│    ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐           │
│    │ V1  │ V2  │ V3  │ V4  │ V5  │ V6  │ V7  │ ... │           │
│    │ PID │ PID │ PID │ PID │ PID │ PID │ PID │     │           │
│    └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘           │
│                                                                 │
│    每个顶点一个 Erlang 进程                                      │
└─────────────────────────────────────────────────────────────────┘
```

### 方案 B：多个 Vertex 由一个 Worker 管理（当前实现）

```
┌─────────────────────────────────────────────────────────────────┐
│                      Master                                     │
├─────────────────────────────────────────────────────────────────┤
│                          │                                      │
│    ┌──────────────┐  ┌──────────────┐  ┌──────────────┐        │
│    │   Worker 0   │  │   Worker 1   │  │   Worker 2   │        │
│    │ ┌───┬───┬───┐│  │ ┌───┬───┬───┐│  │ ┌───┬───┬───┐│        │
│    │ │V1 │V2 │V3 ││  │ │V4 │V5 │V6 ││  │ │V7 │V8 │V9 ││        │
│    │ └───┴───┴───┘│  │ └───┴───┴───┘│  │ └───┴───┴───┘│        │
│    └──────────────┘  └──────────────┘  └──────────────┘        │
│                                                                 │
│    少量 Worker 进程，每个管理多个顶点                             │
└─────────────────────────────────────────────────────────────────┘
```

## 为什么选择方案 B？

### 1. Barrier 同步复杂度

```
方案 A (每顶点一进程):
──────────────────────
Master 需要等待 N 个顶点完成

Barrier = #{expected => 10000, received => 0}  %% 10000 个顶点

每个顶点完成都要通知 Master:
  V1 → Master: done
  V2 → Master: done
  ...
  V10000 → Master: done

消息数量: O(N)，N = 顶点数


方案 B (Worker 管理多顶点):
─────────────────────────
Master 只需等待 K 个 Worker 完成

Barrier = #{expected => 4, received => 0}  %% 4 个 Worker

Worker 内部批量处理后汇报:
  Worker0 → Master: done (处理了 2500 个顶点)
  Worker1 → Master: done (处理了 2500 个顶点)
  Worker2 → Master: done (处理了 2500 个顶点)
  Worker3 → Master: done (处理了 2500 个顶点)

消息数量: O(K)，K = Worker 数（通常 << N）
```

### 2. 消息路由开销

```
方案 A:
──────
顶点间直接发消息，需要知道目标进程 PID

V1 发消息给 V5000:
  1. 查找 V5000 的 PID（需要全局注册表）
  2. 发送消息
  3. V5000 收到后处理

问题:
  - 需要维护 vertex_id → PID 的全局映射
  - 每条消息都是跨进程通信
  - 大量小消息，调度开销大


方案 B:
──────
Worker 内顶点通信是内存操作

V1 发消息给 V3（同一 Worker）:
  1. 直接写入 Worker 的 inbox map
  2. 无进程通信开销

V1 发消息给 V5000（不同 Worker）:
  1. 计算目标 Worker ID: hash(V5000) mod K
  2. 批量发送给目标 Worker
  3. 合并器可以提前聚合消息

优势:
  - 本地顶点通信无开销
  - 跨 Worker 消息可批量发送
  - 合并器减少消息量
```

### 3. 内存开销

```
方案 A (每顶点一进程):
──────────────────────
每个 Erlang 进程最小内存: ~300 bytes（堆 + 栈 + PCB）

10000 个顶点:
  进程开销: 10000 × 300 bytes = 3 MB（仅进程结构）
  + 每个进程的 mailbox
  + 每个进程的 state

100万个顶点:
  进程开销: 1000000 × 300 bytes = 300 MB（仅进程结构）


方案 B (Worker 管理多顶点):
─────────────────────────
4 个 Worker 进程:
  进程开销: 4 × 300 bytes = 1.2 KB
  + 共享的 vertices map
  + 共享的 inbox map

顶点数据存储在 map 中，更紧凑
```

### 4. 调度开销

```
方案 A:
──────
超级步执行时，调度器需要调度 N 个进程

┌─────────────────────────────────────────┐
│  Erlang 调度器                           │
├─────────────────────────────────────────┤
│  运行队列: [V1, V2, V3, ..., V10000]    │
│                                         │
│  每个顶点执行时间很短                     │
│  频繁的上下文切换                         │
│  调度开销占比高                           │
└─────────────────────────────────────────┘


方案 B:
──────
超级步执行时，只需调度 K 个进程

┌─────────────────────────────────────────┐
│  Erlang 调度器                           │
├─────────────────────────────────────────┤
│  运行队列: [W0, W1, W2, W3]             │
│                                         │
│  每个 Worker 执行时间较长                 │
│  批量处理多个顶点                         │
│  调度开销可忽略                           │
└─────────────────────────────────────────┘
```

### 5. 错误隔离（方案 A 的优势）

```
方案 A 的优势:
─────────────
V3 崩溃 → 只影响 V3
其他顶点继续执行

方案 B 的问题:
─────────────
V3 崩溃 → 整个 Worker 崩溃
同一 Worker 的 V1, V2, V3 都丢失

解决方案:
─────────
在 Worker 内部用 try-catch 隔离每个顶点
（参考 pregel_single_vertex_restart.md）
```

## 总结对比表

| 方面 | 每顶点一进程 | Worker 管理多顶点 |
|------|-------------|------------------|
| **Barrier 同步** | O(N) 消息 | O(K) 消息，K << N |
| **消息路由** | 全部跨进程 | 本地内存 + 批量跨进程 |
| **内存开销** | 高（进程结构） | 低（共享 map） |
| **调度开销** | 高（频繁切换） | 低（批量执行） |
| **错误隔离** | ✅ 天然隔离 | ❌ 需要 try-catch |
| **实现复杂度** | 简单 | 中等 |
| **可扩展性** | 受限于进程数 | Worker 数可控 |
| **消息合并** | 困难 | ✅ Worker 内可合并 |
| **批量优化** | 困难 | ✅ 批量处理 |

## 量化对比示例

假设有 10000 个顶点，4 个 Worker：

| 指标 | 每顶点一进程 | Worker 管理多顶点 |
|------|-------------|------------------|
| 进程数 | 10000 | 4 |
| Barrier 消息数 | 10000/超步 | 4/超步 |
| 进程内存开销 | ~3 MB | ~1.2 KB |
| 调度实体数 | 10000 | 4 |
| 本地消息比例 | 0% | ~25%（同 Worker） |

## 结论

选择 **Worker 管理多顶点** 的原因：

1. **性能**：减少进程开销、消息开销、调度开销
2. **可扩展性**：Worker 数量可控，不受顶点数限制
3. **批量优化**：消息合并、批量路由
4. **符合 Pregel 原论文设计**：Google 原始 Pregel 也是这种架构

错误隔离的劣势可以通过 **Worker 内 try-catch** 来弥补（参考 `pregel_single_vertex_restart.md`）。

## 适用场景

| 场景 | 推荐方案 |
|------|---------|
| 大规模图（万级以上顶点） | Worker 管理多顶点 |
| 小规模图（百级顶点） | 两种皆可 |
| 需要极致错误隔离 | 每顶点一进程 |
| 需要高吞吐量 | Worker 管理多顶点 |
| 顶点计算时间长 | 每顶点一进程（利用并行） |
| 顶点计算时间短 | Worker 管理多顶点（减少开销） |
